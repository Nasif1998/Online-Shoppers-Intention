# -*- coding: utf-8 -*-
"""Online Shoppers Final Exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yurNHjFQDXmB_TxP2mnDXosgI1MKwE78
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier, StackingClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)


from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

import gradio as gr

"""# **Data Loading**"""

df = pd.read_csv('online_shoppers_intention.csv')
df.head()

df.shape

df.isnull().sum()

df.columns

df.head(50)

df.dtypes

"""# **Data Preprocessing**

1. Converting target value from bool to int
"""

df['Revenue'] = df['Revenue'].astype(int)

"""2. Converting Weekend value from bool to int"""

if 'Weekend' in df.columns and df['Weekend'].dtype == 'bool':
    df['Weekend'] = df['Weekend'].astype(int)

corr_target = df.select_dtypes(include=np.number).corr()['Revenue'].sort_values(ascending=False)
print(corr_target)

"""3.Feature Engineering: TotalPages"""

if all(c in df.columns for c in ['Administrative', 'Informational', 'ProductRelated']):
  df['TotalPages'] = df['Administrative'] + df['Informational'] + df['ProductRelated']

"""4. Feature Engineering: BounceExitRatio"""

if 'BounceRates' in df.columns and 'ExitRates' in df.columns:
  df['BounceExitRatio'] = df['BounceRates'] / (df['ExitRates'] + 1e-6)

X = df.drop('Revenue', axis=1)
y = df['Revenue']

numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()

numeric_cols

categorical_cols

"""5. Train Test split with stratify"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=42)

"""6. Outlier handling: IQR clipping"""

def iqr_clip_train_test(X_tr : pd.DataFrame, X_te : pd.DataFrame, cols):
  X_tr = X_tr.copy()
  X_te = X_te.copy()

  for col in cols:
    Q1 = X_tr[col].quantile(0.25)
    Q3 = X_tr[col].quantile(0.75)
    IQR = Q3 - Q1

    low = Q1 - 1.5 * IQR
    high = Q3 + 1.5 * IQR

    X_tr[col] = X_tr[col].clip(lower=low, upper=high)
    X_te[col] = X_te[col].clip(lower=low, upper=high)

  return X_tr, X_te

X_train, X_test = iqr_clip_train_test(X_train, X_test, numeric_cols)

"""# **Pipeline Creation**"""

numeric_transformer = Pipeline(
    steps = [
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ]
)

cat_transformer = Pipeline(
    steps = [
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ]
)

preprocessor = ColumnTransformer(
    transformers = [
        ('num', numeric_transformer, numeric_cols),
        ('cat', cat_transformer, categorical_cols)
    ],
    remainder = 'drop'
)

"""# **Primary Model Selection**"""

clf_lr = LogisticRegression(max_iter=2000, class_weight="balanced", random_state=42)

clf_rf = RandomForestClassifier(
    n_estimators=400,
    random_state=42
)

clf_gb = GradientBoostingClassifier(random_state=42)

voting_clf = VotingClassifier(
    estimators=[("lr", clf_lr), ("rf", clf_rf), ("gb", clf_gb)],
    voting="soft",
    n_jobs=-1
)

stacking_clf = StackingClassifier(
    estimators=[("lr", clf_lr), ("rf", clf_rf), ("gb", clf_gb)],
    final_estimator=LogisticRegression(max_iter=2000, class_weight="balanced", random_state=42),
    stack_method="predict_proba",
    n_jobs=-1
)

xgb_clf = XGBClassifier(
    n_estimators=600,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.9,
    colsample_bytree=0.9,
    reg_lambda=1.0,
    random_state=42,
    eval_metric="logloss",
    n_jobs=-1
)

models_to_compare ={
    "Logistic Regression": clf_lr,
    "Random Forest": clf_rf,
    "Gradient Boosting": clf_gb,
    "Voting Classifier": voting_clf,
    "Stacking Classifier": stacking_clf,
    "XGBoost": xgb_clf
}

"""# **Primary Model Selection**"""

compare_rows = []

for name, model in models_to_compare.items():
    pipe_cmp = Pipeline([
        ("preprocessor", preprocessor),
        ("model", model)
    ])

    pipe_cmp.fit(X_train, y_train)

    y_prob = pipe_cmp.predict_proba(X_test)[:, 1]
    y_pred = (y_prob >= 0.5).astype(int)

    compare_rows.append({
        "Model": name,
        "ROC-AUC": roc_auc_score(y_test, y_prob),
        "F1": f1_score(y_test, y_pred),
        "Accuracy": accuracy_score(y_test, y_pred)
    })

result_df = pd.DataFrame(compare_rows).sort_values("ROC-AUC", ascending=False)
print("\n=== MODEL COMPARISON (sorted by ROC-AUC) ===")
print(result_df)

"""# **Best Model Selection**"""

best_name = result_df.iloc[0]["Model"]

best_base_model = models_to_compare[best_name]

best_base_model

"""Multiple classification algorithms were evaluated using ROC-AUC as the primary metric.
The model with the highest ROC-AUC was selected as the primary model, as it demonstrated the best ranking ability for purchase prediction on imbalanced data.

# **Model Training**
"""

final_pipe = Pipeline([
    ("preprocessor", preprocessor),
    ("model", best_base_model)
])

"""# **Cross Validation**"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_scores = cross_val_score(final_pipe, X_train, y_train, scoring="roc_auc", cv=cv)

print('\n=== CROSS VALIDATION (Train) ===')
print("CV ROC-AUC scores:", np.round(cv_scores, 4))
print(f"Mean ROC-AUC = {cv_scores.mean():.4f}")
print(f"Std  ROC-AUC = {cv_scores.std():.4f}")

"""# **Hyperparameter Tuning**"""

xgb_clf = XGBClassifier(
    random_state=42,
    eval_metric="logloss",
    n_jobs=-1
)

param_grid_xgb = {
    'model__n_estimators': [400, 700],
    'model__max_depth': [3, 5],
    'model__learning_rate': [0.03, 0.07],
    'model__subsample': [0.8, 1.0],
    'model__colsample_bytree': [0.8, 1.0]
}

grid = GridSearchCV(
    estimator=final_pipe,
    param_grid=param_grid_xgb,
    scoring="roc_auc",
    cv=cv,
    n_jobs=-1,
    verbose=1
)

grid.fit(X_train, y_train)

"""# **Best Model Selection**"""

best_model = grid.best_estimator_

print("Best parameters:", grid.best_params_)
print("Best CV ROC-AUC:", grid.best_score_)

"""# **Model Performance Evaluation**"""

y_prob = best_model.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

print("\n FINAL TEST METRICS (threshold=0.5)")
print("Accuracy :", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, zero_division=0))
print("Recall   :", recall_score(y_test, y_pred, zero_division=0))
print("F1-score :", f1_score(y_test, y_pred, zero_division=0))
print("ROC-AUC  :", roc_auc_score(y_test, y_prob))

print("\n Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\n Classification Report:\n", classification_report(y_test, y_pred, digits=4))

thresholds = np.arange(0.1, 0.9, 0.05)

best_thr = 0.5
best_f1 = 0

for t in thresholds:
    y_pred_t = (y_prob >= t).astype(int)
    f1 = f1_score(y_test, y_pred_t, zero_division=0)

    if f1 > best_f1:
        best_f1 = f1
        best_thr = t

print("Best Threshold:", best_thr)
print("Best F1-score:", best_f1)

y_pred_best = (y_prob >= best_thr).astype(int)

print("Final metrics at tuned threshold")
print("F1-score:", f1_score(y_test, y_pred_best))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("\nClassification Report:\n", classification_report(y_test, y_pred_best))

"""For the final evaluation, the default threshold of 0.5 was retained to ensure standard, unbiased, and reproducible reporting. Threshold tuning was explored additionally to illustrate the precision recall trade off in this imbalanced dataset."""

import pickle

with open("best_model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print("Final model saved as best_model.pkl")

